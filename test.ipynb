{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xXkrH43GV-sW",
    "outputId": "37a9f19a-251d-40f5-dc9b-54779caf48e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEAzrolWXQNc",
    "outputId": "fe1ef283-9b5b-404d-a777-0dbe6eb634f9"
   },
   "outputs": [],
   "source": [
    "!pip install traker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ONmWcrKn6A0y",
    "outputId": "e2c49982-d960-40e8-ab4a-f21eacc9b4c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torchvision.datasets import Food101, ImageFolder\n",
    "from trak import TRAKer\n",
    "from trak import modelout_functions\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from src.train import train_model\n",
    "from src.early_stopping import EarlyStopping\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ue3_pIWcV1gm",
    "outputId": "33407e57-d744-4b67-dc3a-5320f920ca6d"
   },
   "outputs": [],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-18\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vYAkNawuV1gn"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = transforms.functional.pil_to_tensor(image)\n",
    "    processed_image = processor.preprocess(image)[\"pixel_values\"][0]\n",
    "    return torch.from_numpy(processed_image)\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "train_dataset = Food101(\"data/food-101\", split=\"train\", transform=preprocess_image, download=False)\n",
    "test_dataset = Food101(\"data/food-101\", split=\"test\", transform=preprocess_image, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "filtered_dataset = ImageFolder(\"data/food-101/food-101/images\", transform=preprocess_image)\n",
    "\n",
    "# bruschetta, garlic bread, grilled salmon, omelette, pancakes,\n",
    "# pizza, porkchop, spaghetti bolognese, spaghetti carbonara, steak\n",
    "chosen_indices = [10, 46, 50, 67, 72, 76, 77, 90, 91, 93]\n",
    "new_class_idx_mapping = {k: v for k, v in zip(chosen_indices, range(num_classes))}\n",
    "\n",
    "filtered_dataset.classes = [filtered_dataset.classes[i] for i in chosen_indices]\n",
    "filtered_dataset.class_to_idx = {k: i for i, k in enumerate(filtered_dataset.classes)}\n",
    "\n",
    "print(len(filtered_dataset))\n",
    "\n",
    "filtered_dataset.samples = list(filter(lambda s: s[1] in chosen_indices, filtered_dataset.samples))\n",
    "filtered_dataset.samples = list(map(lambda s: (s[0], new_class_idx_mapping[s[1]]), filtered_dataset.samples))\n",
    "\n",
    "filtered_train_subset, filtered_test_subset = torch.utils.data.random_split(filtered_dataset, [0.8, 0.2], torch.Generator().manual_seed(42))\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(filtered_train_subset, batch_size=64, shuffle=True)\n",
    "test_dl = torch.utils.data.DataLoader(filtered_test_subset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(len(filtered_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = nn.Sequential(\n",
    "                    nn.Flatten(start_dim=1, end_dim=-1),\n",
    "                    nn.Linear(in_features=512, out_features=num_classes))\n",
    "for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "model.num_labels = num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1NmKUAtIV1gt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [02:17<00:00,  1.10s/it]\n",
      "100%|██████████| 32/32 [00:25<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 4.543, Train Acc: 0.614,Valid loss: 1.275 Valid Acc: 0.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:47<00:00,  1.17it/s]\n",
      "100%|██████████| 32/32 [00:23<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Loss: 3.014, Train Acc: 0.745,Valid loss: 1.076 Valid Acc: 0.639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:48<00:00,  1.15it/s]\n",
      "100%|██████████| 32/32 [00:22<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Loss: 2.152, Train Acc: 0.821,Valid loss: 1.024 Valid Acc: 0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:47<00:00,  1.17it/s]\n",
      "100%|██████████| 32/32 [00:23<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Loss: 1.645, Train Acc: 0.861,Valid loss: 1.323 Valid Acc: 0.626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:45<00:00,  1.19it/s]\n",
      "100%|██████████| 32/32 [00:23<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Loss: 1.121, Train Acc: 0.904,Valid loss: 1.344 Valid Acc: 0.616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:45<00:00,  1.18it/s]\n",
      "100%|██████████| 32/32 [00:24<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Loss: 0.793, Train Acc: 0.934,Valid loss: 1.605 Valid Acc: 0.608\n",
      "Early stopping at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.001)\n",
    "\n",
    "train_model(model, train_dl, test_dl, num_epochs, optimizer, early_stopping=early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DEjUV_BrV1gv"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_finetuned_baseline.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "il0FxTk_YxHZ"
   },
   "outputs": [],
   "source": [
    "#finetuned_path = '/content/drive/MyDrive/automating_science/model_finetuned_baseline.pth' #model_finetuned_baseline.pth\n",
    "finetuned_path = \"model_finetuned_baseline.pth\"\n",
    "checkpoint = torch.load(finetuned_path,  map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CA3JgPuEbNLZ"
   },
   "outputs": [],
   "source": [
    "train_dl_no_shuffle = torch.utils.data.DataLoader(filtered_train_subset, batch_size=32, shuffle=False)\n",
    "test_dl_no_shuffle = torch.utils.data.DataLoader(filtered_test_subset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_PYwnkXehUXL"
   },
   "outputs": [],
   "source": [
    "class ResNetOutput(modelout_functions.AbstractModelOutput):\n",
    "    def __init__(self, loss_temperature: float = 1.0):\n",
    "       super().__init__()\n",
    "       self.softmax = nn.Softmax(dim=-1)\n",
    "       self.loss_temperature = loss_temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def get_output(\n",
    "                model: torch.nn.Module,\n",
    "                weights: Iterable[torch.Tensor],\n",
    "                buffers: Iterable[torch.Tensor],\n",
    "                image: torch.Tensor,\n",
    "                label: torch.Tensor\n",
    "      ):\n",
    "      for key, value in weights.items():\n",
    "        weights[key] = weights[key].to(device)\n",
    "      output = torch.func.functional_call(model, (weights, buffers), image.unsqueeze(0))\n",
    "      logits = output.logits #our change\n",
    "      bindex = torch.arange(logits.shape[0]).to(logits.device, non_blocking=False)\n",
    "      logits_correct = logits[bindex, label.unsqueeze(0)]\n",
    "\n",
    "      cloned_logits = logits.clone()\n",
    "\n",
    "      cloned_logits[bindex, label.unsqueeze(0)] = torch.tensor(-torch.inf, device=logits.device, dtype=logits.dtype)\n",
    "\n",
    "      margins = logits_correct - cloned_logits.logsumexp(dim=-1)\n",
    "      return margins.sum()\n",
    "    \n",
    "    def get_out_to_loss_grad(self, model, weights, buffers, batch):\n",
    "      for key, value in weights.items():\n",
    "        weights[key] = weights[key].to(device)\n",
    "      images, labels = batch\n",
    "      output = torch.func.functional_call(model, (weights, buffers), images)\n",
    "      logits = output.logits #our change\n",
    "\n",
    "      ps = self.softmax(logits / self.loss_temperature)[torch.arange(logits.size(0)), labels]\n",
    "      return (1 - ps).clone().detach().unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XN4FUNGV1gx",
    "outputId": "5efe328f-6aea-48ae-86a9-36d1a797d924"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:TRAK:Could not use CudaProjector.\n",
      "Reason: No module named 'fast_jl'\n",
      "ERROR:TRAK:Defaulting to BasicProjector.\n",
      "INFO:STORE:Existing model IDs in C:\\Users\\kamil\\OneDrive\\Pulpit\\przedmioty\\semestr 8\\automating science\\trak-for-automating-science\\trak_results: [0]\n",
      "INFO:STORE:Model IDs that have been finalized: [0]\n",
      "INFO:STORE:Existing TRAK scores:\n",
      "INFO:STORE:test_val: C:\\Users\\kamil\\OneDrive\\Pulpit\\przedmioty\\semestr 8\\automating science\\trak-for-automating-science\\trak_results\\scores\\test_val.mmap\n"
     ]
    }
   ],
   "source": [
    "traker = TRAKer(model=model,\n",
    "                task=ResNetOutput(),\n",
    "                train_set_size=len(train_dl_no_shuffle.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "35EBBpCib2UY"
   },
   "outputs": [],
   "source": [
    "model_id = 0\n",
    "traker.load_checkpoint(checkpoint, model_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "lGhzNkJvV1gy",
    "outputId": "33963680-a114-46ca-d1d2-e1091bf0793c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]c:\\Users\\kamil\\anaconda3\\envs\\ml_gnn\\lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "100%|██████████| 250/250 [24:19<00:00,  5.84s/it]\n"
     ]
    }
   ],
   "source": [
    "for data in tqdm(train_dl_no_shuffle):\n",
    "    data = [xy.cuda() for xy in data]\n",
    "\n",
    "    traker.featurize(batch=data, num_samples=data[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finalizing features for all model IDs..: 100%|██████████| 1/1 [00:00<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "traker.finalize_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [47:07<00:00, 44.88s/it]\n",
      "Finalizing scores for all model IDs..: 100%|██████████| 1/1 [00:00<00:00, 11.92it/s]\n",
      "INFO:STORE:Saving scores in C:\\Users\\kamil\\OneDrive\\Pulpit\\przedmioty\\semestr 8\\automating science\\trak-for-automating-science\\trak_results\\scores/test_val.mmap\n"
     ]
    }
   ],
   "source": [
    "traker.start_scoring_checkpoint(exp_name=\"test_val\", checkpoint=checkpoint, model_id=model_id, num_targets=len(test_dl_no_shuffle.dataset))\n",
    "for batch in tqdm(test_dl_no_shuffle):\n",
    "    batch = [xy.cuda() for xy in batch]\n",
    "    traker.score(batch=batch, num_samples=batch[0].shape[0])\n",
    "\n",
    "test_scores = traker.finalize_scores(exp_name=\"test_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 2000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtest_scores\u001b[49m)\n\u001b[0;32m      4\u001b[0m test_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_scores' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save(\"test_scores.npy\", test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = np.load(\"test_scores.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "\n",
    "for _, y in test_dl_no_shuffle:\n",
    "    targets.extend(y.tolist())\n",
    "targets = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:21<00:00,  2.99it/s]\n"
     ]
    }
   ],
   "source": [
    "def compute_test_dataset_predictions(model, test_dl):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dl):\n",
    "            batch = [xy.cuda() for xy in batch]\n",
    "            output = model(batch[0]).logits\n",
    "            preds.extend(output.tolist())\n",
    "    preds = torch.Tensor(preds)\n",
    "    preds = torch.nn.functional.softmax(preds, dim=-1)\n",
    "    return np.array(preds.tolist())\n",
    "\n",
    "\n",
    "preds = compute_test_dataset_predictions(model, test_dl_no_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15793838, 0.27197625, 0.12155634, 0.01027009, 0.27727489,\n",
       "       0.1507057 , 0.26520708, 0.52252223, 0.46870035, 0.59647187])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://openreview.net/pdf?id=Agekm5fdW3 - section 2.2\n",
    "def compute_class_weights(model, preds, targets):\n",
    "    class_weights = []\n",
    "    for i in range(num_classes):\n",
    "        n_elements = np.sum(targets == i)\n",
    "        class_weights.append(np.exp(1 / n_elements * np.sum(np.log(preds[targets == i][:, i]))))\n",
    "    return np.array(class_weights)\n",
    "\n",
    "class_weights = compute_class_weights(model, preds, targets)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:01<00:00, 6015.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2884"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://openreview.net/pdf?id=Agekm5fdW3 - section 2.2\n",
    "def compute_group_alignment_scores(test_scores, targets, class_weights):\n",
    "    group_alignment_scores = []\n",
    "    n_train_examples = test_scores.shape[0]\n",
    "    n_targets = []\n",
    "    for i in range(num_classes):\n",
    "        n_targets.append(np.sum(targets == i))\n",
    "    n_targets = np.array(n_targets)\n",
    "    for i in tqdm(range(n_train_examples)):\n",
    "        example_score = 0.0\n",
    "        for j in range(num_classes):\n",
    "            scaling_factor = class_weights[j] / n_targets[j]\n",
    "            example_score += scaling_factor * np.sum(test_scores[i, targets == j])\n",
    "        group_alignment_scores.append(example_score)\n",
    "    return np.array(group_alignment_scores)\n",
    "\n",
    "group_alignment_scores = compute_group_alignment_scores(test_scores, targets, class_weights)\n",
    "(group_alignment_scores < 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 7996, 7997, 7998], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_indices_to_keep = np.nonzero(~(group_alignment_scores < 0))[0]\n",
    "example_indices_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_after_trak = torch.utils.data.Subset(filtered_train_subset, example_indices_to_keep)\n",
    "train_dl_after_trak = torch.utils.data.DataLoader(train_data_after_trak, batch_size=64, shuffle=True)\n",
    "\n",
    "model_after_trak = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-18\")\n",
    "model_after_trak.classifier = nn.Sequential(\n",
    "                    nn.Flatten(start_dim=1, end_dim=-1),\n",
    "                    nn.Linear(in_features=512, out_features=num_classes))\n",
    "for param in model_after_trak.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "model_after_trak.num_labels = num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [01:01<00:00,  1.31it/s]\n",
      "100%|██████████| 32/32 [00:19<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 2.747, Train Acc: 0.638,Valid loss: 1.585 Valid Acc: 0.552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:59<00:00,  1.34it/s]\n",
      "100%|██████████| 32/32 [00:19<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Loss: 1.633, Train Acc: 0.788,Valid loss: 1.058 Valid Acc: 0.643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:59<00:00,  1.33it/s]\n",
      "100%|██████████| 32/32 [00:19<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Loss: 1.273, Train Acc: 0.834,Valid loss: 1.440 Valid Acc: 0.565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:59<00:00,  1.34it/s]\n",
      "100%|██████████| 32/32 [00:22<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Loss: 0.868, Train Acc: 0.887,Valid loss: 1.433 Valid Acc: 0.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [01:42<00:00,  1.29s/it]\n",
      "100%|██████████| 32/32 [00:43<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Loss: 0.538, Train Acc: 0.932,Valid loss: 1.730 Valid Acc: 0.617\n",
      "Early stopping at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "optimizer = torch.optim.Adam(model_after_trak.parameters(), lr=1e-3)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.001)\n",
    "\n",
    "train_model(model_after_trak, train_dl_after_trak, test_dl, num_epochs, optimizer, early_stopping=early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# show improved predictions on some examples\n",
    "# shortcut learning / simplicity bias\n",
    "# \"describe your findings. Please find an example where there is a surprising connection between training data and some prediction.\"\n",
    "# maybe choose better k (number of removed examples) - the authors say the heuristic (of choosing everything < 0) results in over-estimated k\n",
    "\n",
    "# write a few-page summary\n",
    "# readme, installation documentation\n",
    "# code cleanup"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
