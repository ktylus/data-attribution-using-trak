{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xXkrH43GV-sW",
    "outputId": "37a9f19a-251d-40f5-dc9b-54779caf48e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEAzrolWXQNc",
    "outputId": "fe1ef283-9b5b-404d-a777-0dbe6eb634f9"
   },
   "outputs": [],
   "source": [
    "!pip install traker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ONmWcrKn6A0y",
    "outputId": "e2c49982-d960-40e8-ab4a-f21eacc9b4c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torchvision.datasets import Food101, ImageFolder\n",
    "from trak import TRAKer\n",
    "from trak import modelout_functions\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from src.train import train_model\n",
    "from src.early_stopping import EarlyStopping\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vYAkNawuV1gn"
   },
   "outputs": [],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-18\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-18\")\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = transforms.functional.pil_to_tensor(image)\n",
    "    processed_image = processor.preprocess(image)[\"pixel_values\"][0]\n",
    "    return torch.from_numpy(processed_image)\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "train_dataset = Food101(\"data/food-101\", split=\"train\", transform=preprocess_image, download=False)\n",
    "test_dataset = Food101(\"data/food-101\", split=\"test\", transform=preprocess_image, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "filtered_dataset = ImageFolder(\"data/food-101/food-101/images\", transform=preprocess_image)\n",
    "\n",
    "# classes:\n",
    "# bruschetta, garlic bread, grilled salmon, omelette, pancakes,\n",
    "# pizza, porkchop, spaghetti bolognese, spaghetti carbonara, steak\n",
    "chosen_indices = [10, 46, 50, 67, 72, 76, 77, 90, 91, 93]\n",
    "new_class_idx_mapping = {k: v for k, v in zip(chosen_indices, range(num_classes))}\n",
    "\n",
    "filtered_dataset.classes = [filtered_dataset.classes[i] for i in chosen_indices]\n",
    "filtered_dataset.class_to_idx = {k: i for i, k in enumerate(filtered_dataset.classes)}\n",
    "\n",
    "filtered_dataset.samples = list(filter(lambda s: s[1] in chosen_indices, filtered_dataset.samples))\n",
    "filtered_dataset.samples = list(map(lambda s: (s[0], new_class_idx_mapping[s[1]]), filtered_dataset.samples))\n",
    "\n",
    "filtered_train_subset, filtered_test_subset = torch.utils.data.random_split(filtered_dataset, [0.8, 0.2], torch.Generator().manual_seed(42))\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(filtered_train_subset, batch_size=64, shuffle=True)\n",
    "test_dl = torch.utils.data.DataLoader(filtered_test_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(len(filtered_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = nn.Sequential(\n",
    "                    nn.Flatten(start_dim=1, end_dim=-1),\n",
    "                    nn.Linear(in_features=512, out_features=num_classes))\n",
    "for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "model.num_labels = num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1NmKUAtIV1gt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:37<00:00,  1.28it/s]\n",
      "100%|██████████| 32/32 [00:22<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 0.903, Train Acc: 0.694,Valid loss: 0.555 Valid Acc: 0.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:35<00:00,  1.30it/s]\n",
      "100%|██████████| 32/32 [00:21<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Loss: 0.290, Train Acc: 0.919,Valid loss: 0.517 Valid Acc: 0.827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:38<00:00,  1.27it/s]\n",
      "100%|██████████| 32/32 [00:23<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Loss: 0.081, Train Acc: 0.990,Valid loss: 0.513 Valid Acc: 0.837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [02:01<00:00,  1.03it/s]\n",
      "100%|██████████| 32/32 [00:35<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Loss: 0.027, Train Acc: 0.998,Valid loss: 0.516 Valid Acc: 0.834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:45<00:00,  1.18it/s]\n",
      "100%|██████████| 32/32 [00:23<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Loss: 0.014, Train Acc: 0.999,Valid loss: 0.513 Valid Acc: 0.841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:45<00:00,  1.19it/s]\n",
      "100%|██████████| 32/32 [00:21<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Loss: 0.007, Train Acc: 1.000,Valid loss: 0.514 Valid Acc: 0.848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:45<00:00,  1.19it/s]\n",
      "100%|██████████| 32/32 [00:23<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Loss: 0.005, Train Acc: 1.000,Valid loss: 0.527 Valid Acc: 0.841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:46<00:00,  1.18it/s]\n",
      "100%|██████████| 32/32 [00:23<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Loss: 0.004, Train Acc: 1.000,Valid loss: 0.525 Valid Acc: 0.840\n",
      "Early stopping at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.001)\n",
    "\n",
    "# baseline results - valid loss: 0.513, valid acc: 0.841\n",
    "train_model(model, train_dl, test_dl, num_epochs, optimizer, early_stopping=early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DEjUV_BrV1gv"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_finetuned_baseline.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "il0FxTk_YxHZ"
   },
   "outputs": [],
   "source": [
    "#finetuned_path = '/content/drive/MyDrive/automating_science/model_finetuned_baseline.pth' #model_finetuned_baseline.pth\n",
    "finetuned_path = \"model_finetuned_baseline.pth\"\n",
    "checkpoint = torch.load(finetuned_path,  map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CA3JgPuEbNLZ"
   },
   "outputs": [],
   "source": [
    "train_dl_no_shuffle = torch.utils.data.DataLoader(filtered_train_subset, batch_size=32, shuffle=False)\n",
    "test_dl_no_shuffle = torch.utils.data.DataLoader(filtered_test_subset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_PYwnkXehUXL"
   },
   "outputs": [],
   "source": [
    "class ResNetOutput(modelout_functions.AbstractModelOutput):\n",
    "    def __init__(self, loss_temperature: float = 1.0):\n",
    "       super().__init__()\n",
    "       self.softmax = nn.Softmax(dim=-1)\n",
    "       self.loss_temperature = loss_temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def get_output(\n",
    "                model: torch.nn.Module,\n",
    "                weights: Iterable[torch.Tensor],\n",
    "                buffers: Iterable[torch.Tensor],\n",
    "                image: torch.Tensor,\n",
    "                label: torch.Tensor\n",
    "      ):\n",
    "      for key, value in weights.items():\n",
    "        weights[key] = weights[key].to(device)\n",
    "      output = torch.func.functional_call(model, (weights, buffers), image.unsqueeze(0))\n",
    "      logits = output.logits #our change\n",
    "      bindex = torch.arange(logits.shape[0]).to(logits.device, non_blocking=False)\n",
    "      logits_correct = logits[bindex, label.unsqueeze(0)]\n",
    "\n",
    "      cloned_logits = logits.clone()\n",
    "\n",
    "      cloned_logits[bindex, label.unsqueeze(0)] = torch.tensor(-torch.inf, device=logits.device, dtype=logits.dtype)\n",
    "\n",
    "      margins = logits_correct - cloned_logits.logsumexp(dim=-1)\n",
    "      return margins.sum()\n",
    "    \n",
    "    def get_out_to_loss_grad(self, model, weights, buffers, batch):\n",
    "      for key, value in weights.items():\n",
    "        weights[key] = weights[key].to(device)\n",
    "      images, labels = batch\n",
    "      output = torch.func.functional_call(model, (weights, buffers), images)\n",
    "      logits = output.logits #our change\n",
    "\n",
    "      ps = self.softmax(logits / self.loss_temperature)[torch.arange(logits.size(0)), labels]\n",
    "      return (1 - ps).clone().detach().unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XN4FUNGV1gx",
    "outputId": "5efe328f-6aea-48ae-86a9-36d1a797d924"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:TRAK:Could not use CudaProjector.\n",
      "Reason: No module named 'fast_jl'\n",
      "ERROR:TRAK:Defaulting to BasicProjector.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STORE:No existing model IDs in C:\\Users\\kamil\\OneDrive\\Pulpit\\przedmioty\\semestr 8\\automating science\\trak-for-automating-science\\trak_results.\n",
      "INFO:STORE:No existing TRAK scores in C:\\Users\\kamil\\OneDrive\\Pulpit\\przedmioty\\semestr 8\\automating science\\trak-for-automating-science\\trak_results.\n"
     ]
    }
   ],
   "source": [
    "traker = TRAKer(model=model,\n",
    "                task=ResNetOutput(),\n",
    "                train_set_size=len(train_dl_no_shuffle.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "35EBBpCib2UY"
   },
   "outputs": [],
   "source": [
    "model_id = 0\n",
    "traker.load_checkpoint(checkpoint, model_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "lGhzNkJvV1gy",
    "outputId": "33963680-a114-46ca-d1d2-e1091bf0793c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]c:\\Users\\kamil\\anaconda3\\envs\\ml_gnn\\lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "100%|██████████| 250/250 [21:01<00:00,  5.04s/it]\n"
     ]
    }
   ],
   "source": [
    "for data in tqdm(train_dl_no_shuffle):\n",
    "    data = [xy.cuda() for xy in data]\n",
    "\n",
    "    traker.featurize(batch=data, num_samples=data[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finalizing features for all model IDs..: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n"
     ]
    }
   ],
   "source": [
    "traker.finalize_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 62/63 [46:18<00:45, 45.35s/it]c:\\Users\\kamil\\anaconda3\\envs\\ml_gnn\\lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "100%|██████████| 63/63 [47:04<00:00, 44.84s/it]\n",
      "Finalizing scores for all model IDs..: 100%|██████████| 1/1 [00:00<00:00,  9.64it/s]\n",
      "INFO:STORE:Saving scores in C:\\Users\\kamil\\OneDrive\\Pulpit\\przedmioty\\semestr 8\\automating science\\trak-for-automating-science\\trak_results\\scores/test_val.mmap\n"
     ]
    }
   ],
   "source": [
    "traker.start_scoring_checkpoint(exp_name=\"test_val\", checkpoint=checkpoint, model_id=model_id, num_targets=len(test_dl_no_shuffle.dataset))\n",
    "for batch in tqdm(test_dl_no_shuffle):\n",
    "    batch = [xy.cuda() for xy in batch]\n",
    "    traker.score(batch=batch, num_samples=batch[0].shape[0])\n",
    "\n",
    "test_scores = traker.finalize_scores(exp_name=\"test_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 2000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"test_scores.npy\", test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = np.load(\"test_scores.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "\n",
    "for _, y in test_dl_no_shuffle:\n",
    "    targets.extend(y.tolist())\n",
    "targets = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:21<00:00,  2.87it/s]\n"
     ]
    }
   ],
   "source": [
    "def compute_test_dataset_predictions(model, test_dl):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dl):\n",
    "            batch = [xy.cuda() for xy in batch]\n",
    "            output = model(batch[0]).logits\n",
    "            preds.extend(output.tolist())\n",
    "    preds = torch.Tensor(preds)\n",
    "    preds = torch.nn.functional.softmax(preds, dim=-1)\n",
    "    return np.array(preds.tolist())\n",
    "\n",
    "\n",
    "preds = compute_test_dataset_predictions(model, test_dl_no_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.65128706, 1.94470773, 1.83594766, 1.96861238, 1.33618492,\n",
       "       1.31584568, 3.12277379, 1.20719328, 1.11670319, 1.97544452])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://openreview.net/pdf?id=Agekm5fdW3 - section 2.2\n",
    "def compute_class_weights(preds, targets):\n",
    "    class_weights = []\n",
    "    for i in range(num_classes):\n",
    "        n_elements = np.sum(targets == i)\n",
    "        # added a minus sign to the whole expression inside exp, as I believe it's incorrect in the original paper\n",
    "        # before, the classes with worse scores got lower weights, but it probably should be the opposite\n",
    "        class_weights.append(np.exp(-1 / n_elements * np.sum(np.log(preds[targets == i][:, i]))))\n",
    "    return np.array(class_weights)\n",
    "\n",
    "class_weights = compute_class_weights(preds, targets)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:01<00:00, 6206.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2888"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://openreview.net/pdf?id=Agekm5fdW3 - section 2.2\n",
    "def compute_class_alignment_scores(test_scores, targets, class_weights):\n",
    "    group_alignment_scores = []\n",
    "    n_train_examples = test_scores.shape[0]\n",
    "    n_targets = []\n",
    "    for i in range(num_classes):\n",
    "        n_targets.append(np.sum(targets == i))\n",
    "    n_targets = np.array(n_targets)\n",
    "    for i in tqdm(range(n_train_examples)):\n",
    "        example_score = 0.0\n",
    "        for j in range(num_classes):\n",
    "            scaling_factor = class_weights[j] / n_targets[j]\n",
    "            example_score += scaling_factor * np.sum(test_scores[i, targets == j])\n",
    "        group_alignment_scores.append(example_score)\n",
    "    return np.array(group_alignment_scores)\n",
    "\n",
    "group_alignment_scores = compute_class_alignment_scores(test_scores, targets, class_weights)\n",
    "(group_alignment_scores < 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7800"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking to remove only a small fraction of hopefully impactful examples\n",
    "sorted_group_alignment_scores = np.sort(group_alignment_scores)\n",
    "top_k_to_remove = 200\n",
    "example_indices_to_keep = np.nonzero(~(group_alignment_scores < sorted_group_alignment_scores[top_k_to_remove]))[0]\n",
    "len(example_indices_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_after_trak = torch.utils.data.Subset(filtered_train_subset, example_indices_to_keep)\n",
    "train_dl_after_trak = torch.utils.data.DataLoader(train_data_after_trak, batch_size=64, shuffle=True)\n",
    "\n",
    "model_after_trak = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-18\")\n",
    "model_after_trak.classifier = nn.Sequential(\n",
    "                    nn.Flatten(start_dim=1, end_dim=-1),\n",
    "                    nn.Linear(in_features=512, out_features=num_classes))\n",
    "for param in model_after_trak.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "model_after_trak.num_labels = num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122/122 [01:43<00:00,  1.18it/s]\n",
      "100%|██████████| 32/32 [00:23<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 0.885, Train Acc: 0.706,Valid loss: 0.586 Valid Acc: 0.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122/122 [01:43<00:00,  1.18it/s]\n",
      "100%|██████████| 32/32 [00:23<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Loss: 0.277, Train Acc: 0.923,Valid loss: 0.540 Valid Acc: 0.821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122/122 [01:40<00:00,  1.22it/s]\n",
      "100%|██████████| 32/32 [00:23<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Loss: 0.077, Train Acc: 0.988,Valid loss: 0.530 Valid Acc: 0.831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122/122 [01:47<00:00,  1.14it/s]\n",
      "100%|██████████| 32/32 [00:23<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Loss: 0.025, Train Acc: 0.999,Valid loss: 0.499 Valid Acc: 0.841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122/122 [01:44<00:00,  1.17it/s]\n",
      "100%|██████████| 32/32 [00:24<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Loss: 0.012, Train Acc: 1.000,Valid loss: 0.528 Valid Acc: 0.836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122/122 [01:44<00:00,  1.17it/s]\n",
      "100%|██████████| 32/32 [00:30<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Loss: 0.007, Train Acc: 1.000,Valid loss: 0.530 Valid Acc: 0.838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122/122 [01:53<00:00,  1.07it/s]\n",
      "100%|██████████| 32/32 [00:24<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Loss: 0.005, Train Acc: 1.000,Valid loss: 0.546 Valid Acc: 0.844\n",
      "Early stopping at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(model_after_trak.parameters(), lr=lr)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.001)\n",
    "\n",
    "# results after removing examples selected by TRAK - \n",
    "train_model(model_after_trak, train_dl_after_trak, test_dl, num_epochs, optimizer, early_stopping=early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_after_trak.state_dict(), \"model_after_trak.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved predictions on some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:37<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "preds_after_trak = compute_test_dataset_predictions(model_after_trak, test_dl_no_shuffle)\n",
    "class_weights_after_trak = compute_class_weights(preds_after_trak, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.73052311, 1.83737455, 1.93016986, 1.84678878, 1.35579164,\n",
       "        1.31084612, 2.39423032, 1.17696158, 1.17069184, 2.20053032]),\n",
       " array([1.65128706, 1.94470773, 1.83594766, 1.96861238, 1.33618492,\n",
       "        1.31584568, 3.12277379, 1.20719328, 1.11670319, 1.97544452]),\n",
       " array([ 0.07923605, -0.10733317,  0.0942222 , -0.1218236 ,  0.01960672,\n",
       "        -0.00499956, -0.72854347, -0.03023169,  0.05398865,  0.2250858 ]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# worst class performance has improved and all the other classes' performance is similar\n",
    "# suggesting that the change in the formula was correct - the results are what they should be\n",
    "class_weights_after_trak, class_weights, class_weights_after_trak - class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:36<00:00,  1.71it/s]\n",
      "100%|██████████| 63/63 [00:44<00:00,  1.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('0.811', '0.838')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the worst class was class number 1 - \"garlic bread\"\n",
    "# now we will examine both models' performance on this class\n",
    "\n",
    "# TODO recall for both models\n",
    "\n",
    "def recall_for_class(model, test_dl, class_id):\n",
    "    model.eval()\n",
    "    n_correct = 0\n",
    "    n_all = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in tqdm(test_dl):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            data = data[labels == class_id]\n",
    "            if len(data) == 0:\n",
    "                continue\n",
    "            preds = model(data).logits\n",
    "            n_correct += preds.argmax(dim=-1).eq(class_id).sum().item()\n",
    "            n_all += len(data)\n",
    "    return n_correct / n_all\n",
    "\n",
    "f\"{recall_for_class(model, test_dl_no_shuffle, 1):.3f}\", f\"{recall_for_class(model_after_trak, test_dl_no_shuffle, 1):.3f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcut learning / simplicity bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surprising / insightful examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# shortcut learning / simplicity bias\n",
    "# \"describe your findings. Please find an example where there is a surprising connection between training data and some prediction.\"\n",
    "\n",
    "# write a few-page summary\n",
    "# readme, installation documentation\n",
    "# code cleanup"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
