{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXkrH43GV-sW",
        "outputId": "37a9f19a-251d-40f5-dc9b-54779caf48e8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install traker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEAzrolWXQNc",
        "outputId": "fe1ef283-9b5b-404d-a777-0dbe6eb634f9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: traker in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from traker) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from traker) (1.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from traker) (4.66.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->traker) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->traker) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->traker) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->traker) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->traker) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->traker) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->traker) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->traker) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->traker) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->traker) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->traker) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->traker) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->traker) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->traker) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->traker) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->traker) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->traker) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->traker) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->traker) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->traker) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->traker) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ONmWcrKn6A0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2c49982-d960-40e8-ab4a-f21eacc9b4c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch import nn as nn\n",
        "from torchvision.datasets import Food101\n",
        "from trak import TRAKer\n",
        "from trak import modelout_functions\n",
        "from collections.abc import Iterable\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue3_pIWcV1gm",
        "outputId": "33407e57-d744-4b67-dc3a-5320f920ca6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-18\")\n",
        "model = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-18\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vYAkNawuV1gn"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image):\n",
        "    image = transforms.functional.pil_to_tensor(image)\n",
        "    processed_image = processor.preprocess(image)[\"pixel_values\"][0]\n",
        "    return torch.from_numpy(processed_image)\n",
        "\n",
        "train_dataset = Food101(\"data/food-101\", split=\"train\", transform=preprocess_image, download=False)\n",
        "test_dataset = Food101(\"data/food-101\", split=\"test\", transform=preprocess_image, download=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pc9PIg03V1go",
        "outputId": "eb67774c-28bb-4f6f-ff04-56b3ca9c71b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 3, 224, 224]), torch.Size([64]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "for X, y in train_dl:\n",
        "    break\n",
        "\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Coq666k5V1gp",
        "outputId": "bcabbae2-1697-457d-ee4c-6e0762eaa46b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNetForImageClassification(\n",
              "  (resnet): ResNetModel(\n",
              "    (embedder): ResNetEmbeddings(\n",
              "      (embedder): ResNetConvLayer(\n",
              "        (convolution): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "        (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (activation): ReLU()\n",
              "      )\n",
              "      (pooler): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (encoder): ResNetEncoder(\n",
              "      (stages): ModuleList(\n",
              "        (0): ResNetStage(\n",
              "          (layers): Sequential(\n",
              "            (0): ResNetBasicLayer(\n",
              "              (shortcut): Identity()\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "            (1): ResNetBasicLayer(\n",
              "              (shortcut): Identity()\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): ResNetStage(\n",
              "          (layers): Sequential(\n",
              "            (0): ResNetBasicLayer(\n",
              "              (shortcut): ResNetShortCut(\n",
              "                (convolution): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "            (1): ResNetBasicLayer(\n",
              "              (shortcut): Identity()\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): ResNetStage(\n",
              "          (layers): Sequential(\n",
              "            (0): ResNetBasicLayer(\n",
              "              (shortcut): ResNetShortCut(\n",
              "                (convolution): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "            (1): ResNetBasicLayer(\n",
              "              (shortcut): Identity()\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (3): ResNetStage(\n",
              "          (layers): Sequential(\n",
              "            (0): ResNetBasicLayer(\n",
              "              (shortcut): ResNetShortCut(\n",
              "                (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "            (1): ResNetBasicLayer(\n",
              "              (shortcut): Identity()\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=512, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eNtkl5z3V1gq"
      },
      "outputs": [],
      "source": [
        "num_classes = 101\n",
        "\n",
        "model.classifier = nn.Sequential(\n",
        "                    nn.Flatten(start_dim=1, end_dim=-1),\n",
        "                    nn.Linear(in_features=512, out_features=num_classes))\n",
        "for param in model.classifier.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "model.num_labels = 101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnVgkUa-V1gr"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience: int, min_delta: float):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.stop = False\n",
        "        self.val_loss_min = 1e10\n",
        "        self.best_parameters = None\n",
        "\n",
        "    def __call__(self, model, val_loss):\n",
        "        if val_loss < self.val_loss_min - self.min_delta:\n",
        "            self.val_loss_min = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.stop = True\n",
        "                self.best_parameters = model.state_dict()\n",
        "\n",
        "    def get_best_model_parameters(self):\n",
        "        return self.best_parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c0-P8NDV1gs"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "        model: torch.nn.Module,\n",
        "        epochs: int,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        early_stopping: EarlyStopping = None,\n",
        "        criterion=None\n",
        "    ):\n",
        "    criterion = criterion or torch.nn.CrossEntropyLoss()\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        train_correct = 0\n",
        "        train_outputs = 0\n",
        "\n",
        "        for i, data in enumerate(tqdm(train_dl), 0):\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_correct += (torch.argmax(outputs.logits, dim=-1) == labels).sum().item()\n",
        "            train_outputs += outputs.logits.shape[0]\n",
        "\n",
        "            running_loss += loss\n",
        "\n",
        "        model.eval()\n",
        "        total_correct = 0\n",
        "        total_outputs = 0\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(tqdm(test_dl), 0):\n",
        "                images, labels = data\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                val_loss += criterion(outputs.logits, labels).item()\n",
        "                correct = (torch.argmax(outputs.logits, dim=-1) == labels).sum().item()\n",
        "\n",
        "                total_correct += correct\n",
        "                total_outputs += outputs.logits.shape[0]\n",
        "\n",
        "        print(f\"[Epoch {epoch + 1}] Loss: {running_loss / i:.3f}, Train Acc: {train_correct/train_outputs:.3f},\" +\n",
        "              f\"Valid loss: {val_loss/len(test_dl):.3f} Valid Acc: {total_correct/total_outputs:.3f}\")\n",
        "\n",
        "        if early_stopping:\n",
        "            early_stopping(model, val_loss)\n",
        "            if early_stopping.stop:\n",
        "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                break\n",
        "\n",
        "    if early_stopping:\n",
        "        model.load_state_dict(early_stopping.get_best_model_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NmKUAtIV1gt"
      },
      "outputs": [],
      "source": [
        "num_epochs = 20\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "early_stopping = EarlyStopping(patience=3, min_delta=0.001)\n",
        "\n",
        "train(model, num_epochs, optimizer, early_stopping=early_stopping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEjUV_BrV1gv"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"model_finetuned_baseline.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMOsjIryV1gv"
      },
      "outputs": [],
      "source": [
        "# TODO finetune the model on food101 dataset -> use TRAK -> finetune again on the base model -> look at the results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_path = '/content/drive/MyDrive/automating_science/model_finetuned_baseline.pth' #model_finetuned_baseline.pth\n",
        "checkpoint = torch.load(finetuned_path,  map_location=device)"
      ],
      "metadata": {
        "id": "il0FxTk_YxHZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0x9zepxV1gx",
        "outputId": "15cffcae-4679-478d-c140-8fc85ab70597"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "model = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-18\")\n",
        "\n",
        "num_classes = 101\n",
        "\n",
        "model.classifier = nn.Sequential(\n",
        "                    nn.Flatten(start_dim=1, end_dim=-1),\n",
        "                    nn.Linear(in_features=512, out_features=num_classes))\n",
        "\n",
        "model.num_labels = num_classes\n",
        "\n",
        "model.load_state_dict(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "jg9rdCW_dIvh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl_no_shuffle = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
        "test_dl_no_shuffle = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "CA3JgPuEbNLZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetOutput(modelout_functions.AbstractModelOutput):\n",
        "    def get_output(\n",
        "                model: torch.nn.Module,\n",
        "                weights: Iterable[torch.Tensor],\n",
        "                buffers: Iterable[torch.Tensor],\n",
        "                image: torch.Tensor,\n",
        "                label: torch.Tensor\n",
        "      ):\n",
        "      for key, value in weights.items():\n",
        "        weights[key] = weights[key].to(device)\n",
        "      output = torch.func.functional_call(model, (weights, buffers), image.unsqueeze(0))\n",
        "      logits = output.logits #our change\n",
        "      bindex = torch.arange(logits.shape[0]).to(logits.device, non_blocking=False)\n",
        "      logits_correct = logits[bindex, label.unsqueeze(0)]\n",
        "\n",
        "      cloned_logits = logits.clone()\n",
        "\n",
        "      cloned_logits[bindex, label.unsqueeze(0)] = torch.tensor(-torch.inf, device=logits.device, dtype=logits.dtype)\n",
        "\n",
        "      margins = logits_correct - cloned_logits.logsumexp(dim=-1)\n",
        "      return margins.sum()\n",
        "    def get_out_to_loss_grad(self, model, weights, buffers, batch):\n",
        "      for key, value in weights.items():\n",
        "        weights[key] = weights[key].to(device)\n",
        "      images, labels = batch\n",
        "      output = torch.func.functional_call(model, (weights, buffers), images)\n",
        "      logits = output.logits #our change\n",
        "\n",
        "      ps = self.softmax(logits / self.loss_temperature)[torch.arange(logits.size(0)), labels]\n",
        "      return (1 - ps).clone().detach().unsqueeze(-1)"
      ],
      "metadata": {
        "id": "_PYwnkXehUXL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XN4FUNGV1gx",
        "outputId": "5efe328f-6aea-48ae-86a9-36d1a797d924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:TRAK:Could not use CudaProjector.\n",
            "Reason: No module named 'fast_jl'\n",
            "ERROR:TRAK:Defaulting to BasicProjector.\n",
            "INFO:STORE:Existing model IDs in /content/trak_results: [0]\n",
            "INFO:STORE:No model IDs in /content/trak_results have been finalized.\n",
            "INFO:STORE:No existing TRAK scores in /content/trak_results.\n"
          ]
        }
      ],
      "source": [
        "traker = TRAKer(model=model,\n",
        "                task=ResNetOutput,\n",
        "                train_set_size=len(train_dl_no_shuffle.dataset))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 0\n",
        "traker.load_checkpoint(torch.load(finetuned_path,  map_location=device), model_id=0)"
      ],
      "metadata": {
        "id": "35EBBpCib2UY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "lGhzNkJvV1gy",
        "outputId": "33963680-a114-46ca-d1d2-e1091bf0793c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 72.00 MiB. GPU  (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 576.00 MiB. GPU ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-f7243dcc5dc9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# using the checkpoint loaded above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtraker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeaturize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trak/traker.py\u001b[0m in \u001b[0;36mfeaturize\u001b[0;34m(self, batch, inds, num_samples)\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_computer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_per_sample_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_model_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trak/gradient_computers.py\u001b[0m in \u001b[0;36mcompute_per_sample_grad\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;31m# map over batch dimensions (hence 0 for each batch dimension, and None for model params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         grads = torch.func.vmap(\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mgrads_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0min_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/apis.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# @functools.wraps(func)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mvmap_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/vmap.py\u001b[0m in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;31m# If chunk_size is not specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     return _flat_vmap(\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_in_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/vmap.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_saved_tensors_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/vmap.py\u001b[0m in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mvmap_increment_nesting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomness\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvmap_level\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mbatched_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_batched_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_in_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmap_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mbatched_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatched_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unwrap_batched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmap_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/apis.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0meager_transforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/eager_transforms.py\u001b[0m in \u001b[0;36mgrad_impl\u001b[0;34m(func, argnums, has_aux, args, kwargs)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgrad_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0margnums_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_and_value_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/vmap.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_saved_tensors_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/eager_transforms.py\u001b[0m in \u001b[0;36mgrad_and_value_impl\u001b[0;34m(func, argnums, has_aux, args, kwargs)\u001b[0m\n\u001b[1;32m   1269\u001b[0m             \u001b[0;31m# NB: need create_graph so that backward pass isn't run in no_grad mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_as_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1271\u001b[0;31m             \u001b[0mflat_grad_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_autograd_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_diff_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1272\u001b[0m             \u001b[0mgrad_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_grad_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/eager_transforms.py\u001b[0m in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff_outputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     grad_inputs = torch.autograd.grad(diff_outputs, inputs, grad_outputs,\n\u001b[0m\u001b[1;32m    146\u001b[0m                                       \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                                       \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         result = _engine_run_backward(\n\u001b[0m\u001b[1;32m    413\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 576.00 MiB. GPU "
          ]
        }
      ],
      "source": [
        "for data in train_dl_no_shuffle:\n",
        "    data = [xy.cuda() for xy in data]\n",
        "\n",
        "    traker.featurize(batch=data, num_samples=data[0].shape[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}